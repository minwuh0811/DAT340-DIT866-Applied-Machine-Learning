{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Working with a dataset with categorical features\n",
    "In Assignment 1A, we didn't have to do much preprocessing, because all the features in the two datasets were numerical. (Actually, in the second dataset, we removed all non-numerical features.) In this assignment, we'll instead consider how to deal with non-numerical features.\n",
    "\n",
    "We'll use the famous Adult dataset. This is a binary classification task, where our task is to predict whether an American individual earns more than $50,000 a year, given a number of numerical and categorical features. (The dataset was extracted from a 1994 census database.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Reading the data \n",
    "Please download the two CSV files, the training set and the test set, and save them into your working directory This is the official train/test split defined by the people who created the dataset. It's the same data as in the the public distribution, except that we converted the format into a standard CSV format. \n",
    "Write code to read the CSV file, for instance by using Pandas as in Assignment 1A. Then split the data into an input part X and an output part Y. The output variable, which the classifier will predict, is called target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "  \n",
    "# Read the CSV file.\n",
    "data_train = pd.read_csv('adult_train.csv')\n",
    "data_test = pd.read_csv('adult_test.csv')\n",
    "data_train_shuffled = data_train.sample(frac=1.0, random_state=0)\n",
    "data_test_shuffled = data_test.sample(frac=1.0, random_state=0)\n",
    "X_train = data_train_shuffled.drop('target', axis=1)\n",
    "Y_train=data_train_shuffled['target'].dropna()\n",
    "X_test = data_test_shuffled.drop('target', axis=1)\n",
    "Y_test=data_test_shuffled['target'].dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Encoding the features as numbers. \n",
    "If you look at the data, you will note that it contains several features with categorical values, such as workclass, education etc. All scikit-learn models work with numerical data internally; this means that the categorical features need to be converted to numbers. The most straightforward way to carry out such a conversion is to use one-hot encoding of the features, also known as dummy variables in statistics. In this approach, we define one new column for each observed value of the feature. \n",
    "Scikit-learn includes a number of tools that can do one-hot encoding of categorical features and we'll see how to use one of them, the DictVectorizer. An alternative approach that is a bit more Pandas-friendly and gives more low-level control is to use the recently introduced ColumnTransformer; if you're interested, you can read an introduction to this approach here. We won't use a ColumnTransformer here because it will make Task 3 in this assignment a bit too annoying to solve. \n",
    "The DictVectorizer is used when we store our features as named attributes in dictionaries. For instance, we could represent one individual from the Adult dataset as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t49.0\n",
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t9.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 20)\t110172.0\n",
      "  (0, 21)\t40.0\n",
      "  (0, 24)\t1.0\n",
      "  (0, 68)\t1.0\n",
      "  (0, 85)\t1.0\n",
      "  (0, 90)\t1.0\n",
      "  (0, 91)\t1.0\n",
      "  (0, 98)\t1.0\n",
      "  (0, 101)\t1.0\n",
      "  (1, 0)\t49.0\n",
      "  (1, 1)\t0.0\n",
      "  (1, 2)\t0.0\n",
      "  (1, 3)\t9.0\n",
      "  (1, 15)\t1.0\n",
      "  (1, 20)\t105431.0\n",
      "  (1, 21)\t40.0\n",
      "  (1, 22)\t1.0\n",
      "  (1, 68)\t1.0\n",
      "  (1, 79)\t1.0\n",
      "  (1, 88)\t1.0\n",
      "  :\t:\n",
      "  (32559, 3)\t13.0\n",
      "  (32559, 13)\t1.0\n",
      "  (32559, 20)\t194636.0\n",
      "  (32559, 21)\t45.0\n",
      "  (32559, 24)\t1.0\n",
      "  (32559, 68)\t1.0\n",
      "  (32559, 81)\t1.0\n",
      "  (32559, 90)\t1.0\n",
      "  (32559, 91)\t1.0\n",
      "  (32559, 98)\t1.0\n",
      "  (32559, 103)\t1.0\n",
      "  (32560, 0)\t25.0\n",
      "  (32560, 1)\t0.0\n",
      "  (32560, 2)\t0.0\n",
      "  (32560, 3)\t8.0\n",
      "  (32560, 6)\t1.0\n",
      "  (32560, 20)\t374163.0\n",
      "  (32560, 21)\t60.0\n",
      "  (32560, 24)\t1.0\n",
      "  (32560, 55)\t1.0\n",
      "  (32560, 76)\t1.0\n",
      "  (32560, 89)\t1.0\n",
      "  (32560, 91)\t1.0\n",
      "  (32560, 98)\t1.0\n",
      "  (32560, 103)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "X_train_dicts = X_train.to_dict('records')\n",
    "X_test_dicts=X_test.to_dict('records')\n",
    "dv = DictVectorizer()\n",
    "print(X_train_dicts)\n",
    "X_train_encoded = dv.fit_transform(X_train_dicts)\n",
    "print(X_train_encoded)\n",
    "X_test_encoded = dv.transform(X_test_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Combining the steps. \n",
    "In the example above, we first transformed the list of dictionaries into a numerical matrix, and then we used this matrix when training the classifier. A separate preprocessing step was carried out for the test set. \n",
    "In machine learning setups, we often use long chains of preprocessing steps. The one-hot encoding is one example, and other such steps might be scaling, feature selection, imputation of missing values, etc. As you can imagine, keeping track of the preprocessing steps can be tedious and error-prone, so it makes sense to handle such preprocessing chains automatically. \n",
    "A Pipeline consists of a sequence of scikit-learn modules. The most convenient way to build a Pipeline is to use the utility function make_pipeline. For instance, to build a pipeline consisting of a vectorization step and then a decision tree classifier.\n",
    "The Pipeline can be treated as any classifier: we can call fit and predict as usual. Concretely, when we call fit on a Pipeline, it will in turn call fit_transform on all intermediate steps and then fit on the final step. When we call predict, transform will be called on the intermediate steps and then predict on the final step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156999174963246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "  DictVectorizer(),\n",
    "  DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "pip=pipeline.fit(X_train_dicts,Y_train)\n",
    "cross_val_score(pip, X_train_dicts,Y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is built including GradientBoostingClassifier together with DictVectorizer to do  one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision trees and random forests\n",
    "In the previous assignment, in one of the optional tasks (Task 4, step 4) we investigated the performance of a regression model as a function of the depth of the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting and overfitting in decision tree classifiers. \n",
    "As the first step, please reproduce this experiment for this dataset, but now using scikit-learn's DecisionTreeClassifier instead of your own regression model. Of course, you should use an evaluation metric for classification, not the mean squared error. Do you see a similar effect now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2e04092fc88>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "score_test=[]\n",
    "score=[]\n",
    "N=[]\n",
    "for i in range(1,20):\n",
    "    pipeline = make_pipeline(\n",
    "      DictVectorizer(),\n",
    "      DecisionTreeClassifier(max_depth=i)\n",
    "    )\n",
    "    pip=pipeline.fit(X_train_dicts,Y_train)\n",
    "    Y_guess = pip.predict(X_test_dicts)\n",
    "    score.append(cross_val_score(pip, X_train_dicts,Y_train, cv=5, scoring='accuracy').mean())\n",
    "    score_test.append(accuracy_score(Y_test, Y_guess))\n",
    "    N.append(i)\n",
    "plt.plot(N,score_test,color='purple',marker='o', markerfacecolor='purple', markersize=4, linewidth=2)\n",
    "plt.plot(N,score,color='green',marker='o', markerfacecolor='green', markersize=4, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting and overfitting in random forest classifiers. \n",
    "Replace the DecisionTreeClassifier with a RandomForestClassifier. \n",
    "The hyperparameter n_estimators defines the number of decision trees used in ensemble. \n",
    "Investigate how the underfitting/overfitting curve is affected by the number of trees. You can investigate ensemble sizes ranging from 1 up until a few hundred. \n",
    "Hint. These experiments can take some time to run. When n_estimators is large, you can reduce the training time quite a bit by adjusting the hyperparameter n_jobs, which will train several trees in parallel. By default, only one CPU core is used, but if you set n_jobs=-1, all cores will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d0f44eef6619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mpip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_dicts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mY_guess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_dicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_dicts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 383\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "score_test=[]\n",
    "score=[]\n",
    "N=[]\n",
    "for i in range(1,501,50):\n",
    "    pipeline = make_pipeline(\n",
    "      DictVectorizer(),\n",
    "      RandomForestClassifier(n_estimators=i, n_jobs=-1)\n",
    "    )\n",
    "    pip=pipeline.fit(X_train_dicts,Y_train)\n",
    "    Y_guess = pip.predict(X_test_dicts)\n",
    "    score.append(cross_val_score(pip, X_train_dicts,Y_train, cv=5, scoring='accuracy').mean())\n",
    "    score_test.append(accuracy_score(Y_test, Y_guess))\n",
    "    N.append(i)\n",
    "plt.plot(N,score_test,color='purple',marker='o', markerfacecolor='purple', markersize=4, linewidth=2)\n",
    "plt.plot(N,score,color='green',marker='o', markerfacecolor='green', markersize=4, linewidth=2)\n",
    "\n",
    "print(score_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things that you might want to discuss in your report: \n",
    "What's the difference between the curve for a decision tree and for a random forest with an ensemble size of 1, and why do we see this difference? \n",
    "\n",
    "The accuracy for random forest at the ensemble size 1 is not same as a decision tree. \n",
    "\n",
    "two reasons:\n",
    "\n",
    "1. Random Forest uses a bootstrapped sub-sample of training set but the decision tree uses a whole training set \n",
    "\n",
    "2. The number of features are used in random forest is equal to the parameter max_features, however, the decision tree uses all the features. \n",
    "\n",
    "What happens with the curve for random forests as the ensemble size grows? \n",
    "\n",
    "as the ensemble size grows, the accuracy is increased and quickly converged.\n",
    "\n",
    "What happens with the best observed test set accuracy as the ensemble size grows? \n",
    "\n",
    "it doesn't largely change the value as the ensemble size grows.\n",
    "\n",
    "What happens with the training time as the ensemble size grows? \n",
    "\n",
    "The traning time is increased as the ensemble size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Feature importances in random forest classifiers\n",
    "Decision trees and random forests are trained by computing importance scores for individual features in different ways: information gain, Gini impurity, variance reduction, etc. \n",
    "As a way to make our classifiers more interpretable, we can print the importance scores. In scikit-learn, decision trees and ensemble classifiers such as random forests all define an attribute called feature_importances_ (note the final underscore in this name). This is a NumPy array that stores the importance scores for each feature column in the training data matrix. For random forests and other tree ensembles, these importance scores are computed by averaging the scores when training all the different trees in the ensemble. \n",
    "To make these importance scores easier to understand, we can use the attribute feature_names_ (note the underscore again) in the DictVectorizer. \n",
    "Sort the features by importance scores in reverse order (so that the most important feature comes first), inspect the first few of these features, and try to reason about why you got this result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "      DictVectorizer(),\n",
    "      RandomForestClassifier(n_estimators=2, n_jobs=-1)\n",
    "    )\n",
    "print(X_train_dicts)\n",
    "pip=pipeline.fit(X_train_dicts,Y_train)\n",
    "names= [name for name in pip.steps[0][1].feature_names_]\n",
    "importances= [importance for importance in pip.steps[1][1].feature_importances_]\n",
    "indices = np.argsort(importances)[::-1]\n",
    "matrix=[importances[i] for i in indices]\n",
    "name_matrix=[names[i] for i in indices]\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(len(indices)), matrix,color=\"r\", align=\"center\")\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"the most important feature is \" + name_matrix[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please also mention an alternative way to compute some sort of importance score of individual features. (You don't need to implement it.) Here, you can either use your common sense, or optionally read the discussion by Parr et al. (2018) that gives some criticism of decision tree-based feature importance scores and discusses some alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop-column importance method can be an alternative way to compute importance score which drop a entire column then compare the influence before and after the dropped column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
