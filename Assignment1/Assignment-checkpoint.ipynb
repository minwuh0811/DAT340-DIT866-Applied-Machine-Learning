{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: A classification example: fetal heart condition diagnosis\n",
    "The UCI Machine Learning Repository contains several datasets that can be used to investigate different machine learning algorithms. In this exercise, we'll use a dataset of fetal heart diagnosis. The dataset contains measurements from about 2,600 fetuses. This is a classification task, where our task is to predict a diagnosis type following the FIGO Intrapartum Fetal Monitoring Guidelines: normal, suspicious, or pathological."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Reading the data\n",
    "\n",
    "This file contains the data that we will use. This file contains the same data as in the public distribution, except that we converted from Excel to CSV. Download the file and save it in a working directory.\n",
    "\n",
    "Open your favorite editor or a Jupyter notebook. To read the CSV file, it is probably easiest to use the Pandas library. Here is a code snippet that carries out the relevant steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "  \n",
    "# Read the CSV file.\n",
    "data = pd.read_csv('CTG.csv', skiprows=1)\n",
    "\n",
    "# Select the relevant numerical columns.\n",
    "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
    "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
    "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
    "data = data[selected_cols].dropna()\n",
    "\n",
    "# Shuffle the dataset.\n",
    "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "X = data_shuffled.drop('NSP', axis=1)\n",
    "\n",
    "# Map the diagnosis code to a human-readable label.\n",
    "def to_label(y):\n",
    "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
    "\n",
    "Y = data_shuffled['NSP'].apply(to_label)\n",
    "\n",
    "\n",
    "# Partition the data into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Training the baseline classifier\n",
    "\n",
    "We can now start to investigate different classifiers.\n",
    "The DummyClassifier is a simple classifier that does not make use of the features: it just returns the most common label in the training set, in this case Spondylolisthesis. The purpose of using such a stupid classifier is as a baseline: a simple classifier that we can try before we move on to more complex classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7805882352941176"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Trying out some different classifiers\n",
    "Replace the DummyClassifier with some more meaningful classifier and run the cross-validation again. Try out a few classifiers and see how much you can improve the cross-validation accuracy. Remember, the accuracy is defined as the proportion of correctly classified instances, and we want this value to be high.\n",
    "\n",
    "\n",
    "Here are some possible options:\n",
    "\n",
    "Tree-based classifiers:\n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "sklearn.ensemble.GradientBoostingClassifier\n",
    "\n",
    "\n",
    "Linear classifiers:\n",
    "\n",
    "sklearn.linear_model.Perceptron\n",
    "\n",
    "sklearn.linear_model.LogisticRegression\n",
    "\n",
    "sklearn.svm.LinearSVC\n",
    "\n",
    "\n",
    "Neural network classifier (will take longer time to train):\n",
    "\n",
    "sklearn.neural_network.MLPClassifier\n",
    "\n",
    "You may also try to tune the hyperparameters of the various classifiers to improve the performance. For instance, the decision tree classifier has a parameter that sets the maximum depth, and in the neural network classifier you can control the number of layers and the number of neurons in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tree-based classifier using DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205882352941176"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc=dtc.fit(Xtrain,Ytrain)\n",
    "cross_val_score(dtc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tree-based classifier using RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9400000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "dfc = RandomForestClassifier(n_estimators=100)\n",
    "dfc=dfc.fit(Xtrain,Ytrain)\n",
    "cross_val_score(dfc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tree-based classifier using GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc=gbc.fit(Xtrain,Ytrain)\n",
    "cross_val_score(gbc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier using Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825294117647059"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "lcp = Perceptron()\n",
    "lcp=lcp.fit(Xtrain,Ytrain)\n",
    "cross_val_score(lcp, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8905882352941177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lclr = LogisticRegression(multi_class='auto',solver='newton-cg', max_iter=5000)\n",
    "lclr=lclr.fit(Xtrain,Ytrain)\n",
    "cross_val_score(lclr, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier using LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\bishe\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505882352941176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "slSVC = LinearSVC(max_iter=10000)\n",
    "slSVC=slSVC.fit(Xtrain,Ytrain)\n",
    "cross_val_score(slSVC, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8911764705882353"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier()\n",
    "mlpc= mlpc.fit(Xtrain,Ytrain)\n",
    "cross_val_score(mlpc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Final evaluation\n",
    "When you have found a classifier that gives a high accuracy in the cross-validation evaluation, train it on the whole training set and evaluate it on the held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to step 3, the best results were made using tree-based classifier GradientBoostingClassifier and the accuracy is about 0.93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9295774647887324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Yguess = gbc.predict(Xtest)\n",
    "print(accuracy_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision trees for classification\n",
    "Download the code that was shown during the lecture and use the defined class TreeClassifier as your classifier in an experiment similar to those in Task 1. Tune the hyperparameter max_depth to get the best cross-validation performance, and then evaluate the classifier on the test set.\n",
    "\n",
    "For the report. In your submitted report, please mention what value of max_depth you selected and what accuracy you got.\n",
    "\n",
    "For illustration, let's also draw a tree. Set max_depth to a reasonable small value, and then call draw_tree to visualize the learned decision tree. Include this tree in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tree-based classifier using RandomForestClassifier with different max_depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DecisionTreeLeaf:\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        graph.node(node_id, str(self.value), style='filled')\n",
    "        return node_counter+1, node_id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter+1, node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DecisionTree(ABC, BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        \n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "    \n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)        \n",
    "\n",
    "    # This is the recursive training \n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be \n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "    \n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:,feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "    \n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'maj_sum':\n",
    "            self.criterion_function = majority_sum_scorer\n",
    "        elif self.criterion == 'info_gain':\n",
    "            self.criterion_function = info_gain_scorer\n",
    "        elif self.criterion == 'gini':\n",
    "            self.criterion_function = gini_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Counter(Y)\n",
    "        return self.class_distribution.most_common(1)[0][0]\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return len(self.class_distribution) == 1\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        XY = sorted(zip(X[:, feature], Y))\n",
    "\n",
    "        n = len(XY)\n",
    "\n",
    "        # The frequency tables corresponding to the parts *before and including*\n",
    "        # and *after* the current element.\n",
    "        low_distr = Counter()\n",
    "        high_distr = Counter(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = XY[i][0]\n",
    "            y_i = XY[i][1]\n",
    "\n",
    "            # Update the frequency tables.\n",
    "            low_distr[y_i] += 1\n",
    "            high_distr[y_i] -= 1\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            x_next = XY[i+1][0]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(XY[max_i][0] + XY[max_i+1][0])\n",
    "        return score, feature, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "    \n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return 1-sum(p**2 for p in ps)\n",
    "    \n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "score=[]\n",
    "N=[]\n",
    "for i in range(2,30):\n",
    "    tc=TreeClassifier(max_depth=i)  \n",
    "    tc.fit(Xtrain, Ytrain)\n",
    "    score.append(cross_val_score(tc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean())\n",
    "    N.append(i)\n",
    "plt.plot(N,score,color='purple',marker='o', markerfacecolor='purple', markersize=4, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best cross-validation performance is obtained at the hyperparameter max_depth 18, and then evaluate the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8732394366197183\n"
     ]
    }
   ],
   "source": [
    "tc=TreeClassifier(max_depth=18)  \n",
    "tc.fit(Xtrain, Ytrain)\n",
    "Yguess = tc.predict(Xtest)\n",
    "print(accuracy_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the classifier in the test set is 0.87. The accuracy is largely increased at the max_depth reaching 4. The learned decision tree is visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"641pt\" height=\"392pt\"\r\n",
       " viewBox=\"0.00 0.00 641.49 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 637.493,-388 637.493,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"35.7468\" cy=\"-18\" rx=\"35.9954\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"35.7468\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normal</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"137.747\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M172.247,-123C172.247,-123 97.2468,-123 97.2468,-123 91.2468,-123 85.2468,-117 85.2468,-111 85.2468,-111 85.2468,-99 85.2468,-99 85.2468,-93 91.2468,-87 97.2468,-87 97.2468,-87 172.247,-87 172.247,-87 178.247,-87 184.247,-93 184.247,-99 184.247,-99 184.247,-111 184.247,-111 184.247,-117 178.247,-123 172.247,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"134.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ASTV &gt; 79.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;0 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>2&#45;&gt;0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.713,-86.799C99.1752,-73.4587 77.5477,-54.8897 60.8547,-40.5573\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9718,-37.7619 53.1046,-33.9031 58.4118,-43.0729 62.9718,-37.7619\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"106.747\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>2&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.354,-86.799C135.765,-75.1626 136.316,-59.5479 136.786,-46.2368\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.286,-46.2927 137.141,-36.1754 133.29,-46.0457 140.286,-46.2927\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"150.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"245.747\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"245.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"350.747\" cy=\"-18\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"350.747\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M289.247,-123C289.247,-123 214.247,-123 214.247,-123 208.247,-123 202.247,-117 202.247,-111 202.247,-111 202.247,-99 202.247,-99 202.247,-93 208.247,-87 214.247,-87 214.247,-87 289.247,-87 289.247,-87 295.247,-87 301.247,-93 301.247,-99 301.247,-99 301.247,-111 301.247,-111 301.247,-117 295.247,-123 289.247,-123\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"251.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ALTV &gt; 68.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>5&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.533,-86.799C249.711,-75.1626 248.609,-59.5479 247.669,-46.2368\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.155,-45.9041 246.959,-36.1754 244.172,-46.3971 251.155,-45.9041\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"264.747\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>5&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271.781,-86.799C287.027,-73.7088 308.137,-55.5842 324.695,-41.3679\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"327.105,-43.9118 332.412,-34.7421 322.545,-38.6008 327.105,-43.9118\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"321.247\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M230.247,-210C230.247,-210 155.247,-210 155.247,-210 149.247,-210 143.247,-204 143.247,-198 143.247,-198 143.247,-186 143.247,-186 143.247,-180 149.247,-174 155.247,-174 155.247,-174 230.247,-174 230.247,-174 236.247,-174 242.247,-180 242.247,-186 242.247,-186 242.247,-198 242.247,-198 242.247,-204 236.247,-210 230.247,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"192.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ALTV &gt; 37.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;2 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>6&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.01,-173.799C172.755,-161.702 161.567,-145.305 152.26,-131.667\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"154.994,-129.463 146.467,-123.175 149.212,-133.408 154.994,-129.463\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"182.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;5 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>6&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204.686,-173.799C213.083,-161.702 224.465,-145.305 233.931,-131.667\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.998,-133.386 239.825,-123.175 231.248,-129.395 236.998,-133.386\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"240.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"308.747\" cy=\"-192\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"308.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M333.747,-297C333.747,-297 283.747,-297 283.747,-297 277.747,-297 271.747,-291 271.747,-285 271.747,-285 271.747,-273 271.747,-273 271.747,-267 277.747,-261 283.747,-261 283.747,-261 333.747,-261 333.747,-261 339.747,-261 345.747,-267 345.747,-273 345.747,-273 345.747,-285 345.747,-285 345.747,-291 339.747,-297 333.747,-297\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"308.747\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DS &gt; 0.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;6 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>8&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M285.272,-260.799C267.741,-247.953 243.593,-230.258 224.363,-216.167\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.321,-213.263 216.186,-210.175 222.184,-218.909 226.321,-213.263\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"272.747\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;7 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>8&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308.747,-260.799C308.747,-249.163 308.747,-233.548 308.747,-220.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.247,-220.175 308.747,-210.175 305.247,-220.175 312.247,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"322.247\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"423.747\" cy=\"-192\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"423.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"479.747\" cy=\"-105\" rx=\"38.1938\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"479.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">suspect</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"584.747\" cy=\"-105\" rx=\"48.9926\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"584.747\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pathologic</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M562.747,-210C562.747,-210 502.747,-210 502.747,-210 496.747,-210 490.747,-204 490.747,-198 490.747,-198 490.747,-186 490.747,-186 490.747,-180 496.747,-174 502.747,-174 502.747,-174 562.747,-174 562.747,-174 568.747,-174 574.747,-180 574.747,-186 574.747,-186 574.747,-198 574.747,-198 574.747,-204 568.747,-210 562.747,-210\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"532.747\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Min &gt; 72.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;10 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>12&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M522.021,-173.799C514.379,-161.543 503.985,-144.873 495.416,-131.13\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"498.357,-129.231 490.096,-122.597 492.417,-132.934 498.357,-129.231\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"524.747\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;11 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>12&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M543.27,-173.799C550.648,-161.738 560.641,-145.403 568.97,-131.79\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"572.184,-133.242 574.417,-122.885 566.213,-129.589 572.184,-133.242\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"576.247\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M466.747,-297C466.747,-297 380.747,-297 380.747,-297 374.747,-297 368.747,-291 368.747,-285 368.747,-285 368.747,-273 368.747,-273 368.747,-267 374.747,-261 380.747,-261 380.747,-261 466.747,-261 466.747,-261 472.747,-261 478.747,-267 478.747,-273 478.747,-273 478.747,-285 478.747,-285 478.747,-291 472.747,-297 466.747,-297\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"423.747\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Median &gt; 128.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;9 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>13&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M423.747,-260.799C423.747,-249.163 423.747,-233.548 423.747,-220.237\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"427.247,-220.175 423.747,-210.175 420.247,-220.175 427.247,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"438.747\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>13&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M445.805,-260.799C462.204,-248.01 484.765,-230.417 502.795,-216.357\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.988,-219.085 510.722,-210.175 500.684,-213.565 504.988,-219.085\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"499.247\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M398.747,-384C398.747,-384 348.747,-384 348.747,-384 342.747,-384 336.747,-378 336.747,-372 336.747,-372 336.747,-360 336.747,-360 336.747,-354 342.747,-348 348.747,-348 348.747,-348 398.747,-348 398.747,-348 404.747,-348 410.747,-354 410.747,-360 410.747,-360 410.747,-372 410.747,-372 410.747,-378 404.747,-384 398.747,-384\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"373.747\" y=\"-362.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DP &gt; 1.5?</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;8 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>14&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M360.593,-347.799C351.254,-335.587 338.564,-318.992 328.077,-305.278\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"330.736,-302.993 321.881,-297.175 325.175,-307.245 330.736,-302.993\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"360.747\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;13 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>14&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.865,-347.799C390.913,-335.817 400.443,-319.617 408.419,-306.057\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.59,-307.569 413.644,-297.175 405.557,-304.02 411.59,-307.569\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"415.247\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x2d77a48b358>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc=TreeClassifier(max_depth=4)  \n",
    "tc.fit(Xtrain, Ytrain)\n",
    "tc.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: A regression example: predicting apartment prices\n",
    "Here is another dataset. This dataset was created by Sberbank and contains some statistics from the Russian real estate market. Here is the Kaggle page where you can find the original data.\n",
    "\n",
    "Since we will just be able to handle numerical features and not symbolic ones, we'll need with a simplified version of the dataset. So we'll just select 9 of the columns in the dataset. The goal is to predict the price of an apartment, given numerical information such as the number of rooms, the size of the apartment in square meters, the floor, etc. Our approach will be similar to what we did in the classification example: load the data, find a suitable model using cross-validation over the training set, and finally evaluate on the held-out test data.\n",
    "\n",
    "The following code snippet will carry out the basic reading and preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25252    15.632106\n",
       "9943     15.747032\n",
       "18040    16.077274\n",
       "8625     16.147654\n",
       "13495    15.424948\n",
       "           ...    \n",
       "19720    15.461088\n",
       "20933    15.615569\n",
       "11251    15.424948\n",
       "20113    15.967273\n",
       "27182    15.925724\n",
       "Name: price_doc, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv('sberbank.csv')\n",
    "\n",
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Separate the input and output columns.\n",
    "X = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
    "# Split into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "Y.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a baseline dummy regressor (which always predicts the same value) and evaluate it in a cross-validation setting.\n",
    "\n",
    "This example looks quite similar to the classification example above. The main differences are (a) that we are predicting numerical values, not symbolic values; (b) that we are evaluating using the mean squared error metric, not the accuracy metric that we used to evaluate the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00299215, 0.00299144, 0.00302887, 0.00295424, 0.00298643]),\n",
       " 'score_time': array([0.00099611, 0.00099826, 0.        , 0.        , 0.        ]),\n",
       " 'test_score': array([-0.53020068, -0.49328883, -0.50734896, -0.51602643, -0.52635805])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "m1 = DummyRegressor()\n",
    "cross_validate(m1, Xtrain, Ytrain.astype('int'), scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the dummy regressor with something more meaningful and iterate until you cannot improve the performance. Please note that the cross_validate function returns the negative mean squared error.\n",
    "\n",
    "Some possible regression models that you can try:\n",
    "\n",
    "sklearn.linear_model.LinearRegression\n",
    "\n",
    "sklearn.linear_model.Ridge\n",
    "\n",
    "sklearn.linear_model.Lasso\n",
    "\n",
    "sklearn.tree.DecisionTreeRegressor\n",
    "\n",
    "sklearn.ensemble.RandomForestRegressor\n",
    "\n",
    "sklearn.ensemble.GradientBoostingRegressor\n",
    "\n",
    "sklearn.neural_network.MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "-0.41039288417465175\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "-0.410391695934144\n",
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "-0.40970335588189366\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "-0.728853094486445\n",
      "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False)\n",
      "-0.3870346231303928\n",
      "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                          n_iter_no_change=None, presort='deprecated',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "-0.3651750986618709\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "             warm_start=False)\n",
      "-119.97155317147406\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "classifiers = [\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.Ridge(),\n",
    "    linear_model.Lasso(),\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    neural_network.MLPRegressor()\n",
    "]\n",
    "\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    print(cross_validate(clf, Xtrain, Ytrain.astype('int'), scoring='neg_mean_squared_error', cv=3)['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the negative mean squared error, the best results were made using tree-based classifier GradientBoostingClassifier and the error is about -0,37. \n",
    "\n",
    "Finally, train on the full training set and evaluate on the held-out test set, the mean squared error is 0.27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27140227439665326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "clf=ensemble.GradientBoostingRegressor() \n",
    "clf.fit(Xtrain, Ytrain)\n",
    "mean_squared_error(Ytest, clf.predict(Xtest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
